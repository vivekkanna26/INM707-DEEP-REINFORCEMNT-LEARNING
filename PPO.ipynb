{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3JyRppqErs4g",
    "outputId": "c888b38d-dba2-4366-c579-daea1063fb52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_pc2NprQrtiw"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ym7nI2J4rykf"
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fqV-V5I-r1gW"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ValueNetwork():\n",
    "    def __init__(self, num_features, hidden_size, learning_rate=.01):\n",
    "        self.num_features = num_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.tf_graph = tf.Graph()\n",
    "        with self.tf_graph.as_default():\n",
    "            self.session = tf.Session()\n",
    "\n",
    "            self.observations = tf.placeholder(shape=[None, self.num_features], dtype=tf.float32)\n",
    "            self.W = [\n",
    "                tf.get_variable(\"W1\", shape=[self.num_features, self.hidden_size]),\n",
    "                tf.get_variable(\"W2\", shape=[self.hidden_size, 1])\n",
    "            ]\n",
    "            self.layer_1 = tf.nn.sigmoid(tf.matmul(self.observations, self.W[0]))\n",
    "            self.output = tf.reshape(tf.matmul(self.layer_1, self.W[1]), [-1])\n",
    "\n",
    "            self.rollout = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "            self.loss = tf.losses.mean_squared_error(self.output, self.rollout)\n",
    "            self.grad_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.minimize = self.grad_optimizer.minimize(self.loss)\n",
    "\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.session.run(init)\n",
    "\n",
    "    def get(self, states):\n",
    "        value = self.session.run(self.output, feed_dict={self.observations: states})\n",
    "        return value\n",
    "\n",
    "    def update(self, states, discounted_rewards):\n",
    "        _, loss = self.session.run([self.minimize, self.loss], feed_dict={\n",
    "            self.observations: states, self.rollout: discounted_rewards\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6wdL7R8Ir31F"
   },
   "outputs": [],
   "source": [
    "class PPOPolicyNetwork():\n",
    "    def __init__(self, num_features, layer_1_size, layer_2_size, layer_3_size, num_actions, epsilon=.2,\n",
    "                 learning_rate=9e-4):\n",
    "        self.tf_graph = tf.Graph()\n",
    "\n",
    "        with self.tf_graph.as_default():\n",
    "            self.session = tf.Session()\n",
    "\n",
    "            self.observations = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\n",
    "            self.W = [\n",
    "                tf.get_variable(\"W1\", shape=[num_features, layer_1_size], initializer=tf.contrib.layers.xavier_initializer()),\n",
    "                tf.get_variable(\"W2\", shape=[layer_1_size, layer_2_size], initializer=tf.contrib.layers.xavier_initializer()),\n",
    "                tf.get_variable(\"W3\", shape=[layer_2_size, layer_3_size], initializer=tf.contrib.layers.xavier_initializer()),\n",
    "                tf.get_variable(\"W4\", shape=[layer_3_size, num_actions], initializer=tf.contrib.layers.xavier_initializer())\n",
    "            ]\n",
    "            \n",
    "            self.output = tf.nn.relu(tf.matmul(self.observations, self.W[0]))\n",
    "            self.output = tf.nn.relu(tf.matmul(self.output, self.W[1]))\n",
    "            self.output = tf.nn.relu(tf.matmul(self.output, self.W[2]))\n",
    "            self.output = tf.nn.softmax(tf.matmul(self.output, self.W[3]))\n",
    "\n",
    "            self.advantages = tf.placeholder(shape=[None], dtype=tf.float32)\n",
    "\n",
    "            self.chosen_actions = tf.placeholder(shape=[None, num_actions], dtype=tf.float32)\n",
    "            self.old_probabilities = tf.placeholder(shape=[None, num_actions], dtype=tf.float32)\n",
    "\n",
    "            self.new_responsible_outputs = tf.reduce_sum(self.chosen_actions*self.output, axis=1)\n",
    "            self.old_responsible_outputs = tf.reduce_sum(self.chosen_actions*self.old_probabilities, axis=1)\n",
    "\n",
    "            self.ratio = self.new_responsible_outputs/self.old_responsible_outputs\n",
    "\n",
    "            self.loss = tf.reshape(\n",
    "                            tf.minimum(\n",
    "                                tf.multiply(self.ratio, self.advantages), \n",
    "                                tf.multiply(tf.clip_by_value(self.ratio, 1-epsilon, 1+epsilon), self.advantages)),\n",
    "                            [-1]\n",
    "                        )\n",
    "            self.loss = -tf.reduce_mean(self.loss)\n",
    "\n",
    "            self.W0_grad = tf.placeholder(dtype=tf.float32)\n",
    "            self.W1_grad = tf.placeholder(dtype=tf.float32)\n",
    "            self.W2_grad = tf.placeholder(dtype=tf.float32)\n",
    "            self.W3_grad = tf.placeholder(dtype=tf.float32)\n",
    "\n",
    "            self.gradient_placeholders = [self.W0_grad, self.W1_grad, self.W2_grad, self.W3_grad]\n",
    "            self.trainable_vars = self.W\n",
    "            self.gradients = [(np.zeros(var.get_shape()), var) for var in self.trainable_vars]\n",
    "\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.get_grad = self.optimizer.compute_gradients(self.loss, self.trainable_vars)\n",
    "            self.apply_grad = self.optimizer.apply_gradients(zip(self.gradient_placeholders, self.trainable_vars))\n",
    "            init = tf.global_variables_initializer()\n",
    "            self.session.run(init)\n",
    "\n",
    "    def get_dist(self, states):\n",
    "        dist = self.session.run(self.output, feed_dict={self.observations: states})\n",
    "        return dist\n",
    "\n",
    "    def update(self, states, chosen_actions, ep_advantages):\n",
    "        old_probabilities = self.session.run(self.output, feed_dict={self.observations: states})\n",
    "        self.session.run(self.apply_grad, feed_dict={\n",
    "            self.W0_grad: self.gradients[0][0],\n",
    "            self.W1_grad: self.gradients[1][0],\n",
    "            self.W2_grad: self.gradients[2][0],\n",
    "            self.W3_grad: self.gradients[3][0],\n",
    "        })\n",
    "        self.gradients, loss = self.session.run([self.get_grad, self.output], feed_dict={\n",
    "            self.observations: states,\n",
    "            self.advantages: ep_advantages,\n",
    "            self.chosen_actions: chosen_actions,\n",
    "            self.old_probabilities: old_probabilities\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "GTEtCLdZr8An"
   },
   "outputs": [],
   "source": [
    "class PPO():\n",
    "    def __init__(self, env, num_features=1, num_actions=1, gamma=.98, lam=1, epsilon=.2,\n",
    "                 value_network_lr=0.1, policy_network_lr=9e-4, value_network_hidden_size=100,\n",
    "                 policy_network_hidden_size_1=10, policy_network_hidden_size_2=10, policy_network_hidden_size_3=10):\n",
    "        \n",
    "        self.env = env\n",
    "        self.num_features = num_features\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.Pi = PPOPolicyNetwork(num_features=num_features, num_actions=num_actions, \n",
    "                                   layer_1_size=policy_network_hidden_size_1,\n",
    "                                   layer_2_size=policy_network_hidden_size_2,\n",
    "                                   layer_3_size=policy_network_hidden_size_3,\n",
    "                                   epsilon=epsilon,\n",
    "                                   learning_rate=policy_network_lr)\n",
    "        self.V = ValueNetwork(num_features, value_network_hidden_size, learning_rate=value_network_lr)\n",
    "\n",
    "    def discount_rewards(self, rewards):\n",
    "        running_total = 0\n",
    "        discounted = np.zeros_like(rewards)\n",
    "        for r in reversed(range(len(rewards))):\n",
    "            running_total = running_total * self.gamma + rewards[r]\n",
    "            discounted[r] = running_total\n",
    "        return discounted\n",
    "\n",
    "    def calculate_advantages(self, rewards, values):\n",
    "        advantages = np.zeros_like(rewards)\n",
    "        for t in range(len(rewards)):\n",
    "            ad = 0\n",
    "            for l in range(0, len(rewards) - t - 1):\n",
    "                delta = rewards[t+l] + self.gamma*values[t+l+1] - values[t+l]\n",
    "                ad += ((self.gamma*self.lam)**l)*(delta)\n",
    "            ad += ((self.gamma*self.lam)**l)*(rewards[t+l] - values[t+l])\n",
    "            advantages[t] = ad\n",
    "        return (advantages - np.mean(advantages))/np.std(advantages)\n",
    "\n",
    "\n",
    "    def run_model(self):\n",
    "        episode = 1\n",
    "        running_reward = []\n",
    "        step = 0\n",
    "        render = False\n",
    "        while(True):\n",
    "            s0 = self.env.reset()\n",
    "            is_terminal = False\n",
    "            ep_rewards = []\n",
    "            ep_actions = []\n",
    "            ep_states = []\n",
    "            score = 0\n",
    "            while not is_terminal:\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "                action = np.random.choice(range(self.num_actions), p=self.Pi.get_dist(np.array(s0)[np.newaxis, :])[0])\n",
    "                a_binarized = np.zeros(self.num_actions)\n",
    "                a_binarized[action] = 1\n",
    "                s1, r, is_terminal, _ = self.env.step(action)\n",
    "                score += r\n",
    "                ep_actions.append(a_binarized)\n",
    "                ep_rewards.append(r)\n",
    "                ep_states.append(s0)\n",
    "                s0 = s1\n",
    "                if is_terminal:\n",
    "                    ep_actions = np.vstack(ep_actions)\n",
    "                    ep_rewards = np.array(ep_rewards, dtype=np.float_)\n",
    "                    ep_states = np.vstack(ep_states)\n",
    "                    targets = self.discount_rewards(ep_rewards)\n",
    "                    for i in range(len(ep_states)):\n",
    "                        self.V.update([ep_states[i]], [targets[i]])\n",
    "                    ep_advantages = self.calculate_advantages(ep_rewards, self.V.get(ep_states))\n",
    "                    vs = self.V.get(ep_states)\n",
    "                    self.Pi.update(ep_states, ep_actions, ep_advantages)\n",
    "                    ep_rewards = []\n",
    "                    ep_actions = []\n",
    "                    ep_states = []\n",
    "                    running_reward.append(score)\n",
    "                    if episode % 25 == 0:\n",
    "                        avg_score = np.mean(running_reward[-25:])\n",
    "                        print(\"Episode: \" + str(episode) + \" Score: \" + str(avg_score))\n",
    "                        if avg_score >= 500:\n",
    "                            print(\"Solved!\")\n",
    "                            render = True\n",
    "                    episode += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "t2tMtT8er-MR",
    "outputId": "45b77e09-1ad8-44b4-936f-9f7fa9f9d324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Episode: 25 Score: 26.36\n",
      "Episode: 50 Score: 29.72\n",
      "Episode: 75 Score: 144.96\n",
      "Episode: 100 Score: 340.56\n",
      "Episode: 125 Score: 218.68\n",
      "Episode: 150 Score: 422.36\n",
      "Episode: 175 Score: 386.84\n",
      "Episode: 200 Score: 334.08\n",
      "Episode: 225 Score: 314.16\n",
      "Episode: 250 Score: 309.8\n",
      "Episode: 275 Score: 323.72\n",
      "Episode: 300 Score: 250.32\n",
      "Episode: 325 Score: 383.32\n",
      "Episode: 350 Score: 489.76\n",
      "Episode: 375 Score: 392.44\n",
      "Episode: 400 Score: 326.4\n",
      "Episode: 425 Score: 312.88\n",
      "Episode: 450 Score: 395.52\n",
      "Episode: 475 Score: 485.24\n",
      "Episode: 500 Score: 483.96\n",
      "Episode: 525 Score: 446.48\n",
      "Episode: 550 Score: 432.96\n",
      "Episode: 575 Score: 382.84\n",
      "Episode: 600 Score: 495.48\n",
      "Episode: 625 Score: 448.16\n",
      "Episode: 650 Score: 411.6\n",
      "Episode: 675 Score: 258.28\n",
      "Episode: 700 Score: 174.84\n",
      "Episode: 725 Score: 242.56\n",
      "Episode: 750 Score: 352.24\n",
      "Episode: 775 Score: 409.92\n",
      "Episode: 800 Score: 420.48\n",
      "Episode: 825 Score: 456.32\n",
      "Episode: 850 Score: 431.0\n",
      "Episode: 875 Score: 496.44\n",
      "Episode: 900 Score: 482.08\n",
      "Episode: 925 Score: 493.48\n",
      "Episode: 950 Score: 477.08\n",
      "Episode: 975 Score: 500.0\n",
      "Solved!\n"
     ]
    },
    {
     "ename": "NoSuchDisplayException",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1c53b8aabaaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0mvalue_network_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_network_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_network_hidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             policy_network_hidden_size_1=40, policy_network_hidden_size_2=35, policy_network_hidden_size_3=30)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-2696fe854277>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_terminal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0ma_binarized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;31m# trickery is for circular import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0m_pyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1878\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pyglet_doc_run\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m     \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m     \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_shadow_window\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m_create_shadow_window\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0m_shadow_window\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWindow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisible\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0m_shadow_window\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/xlib/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_handlers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXlibWindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;32mglobal\u001b[0m \u001b[0m_can_detect_autorepeat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, width, height, caption, resizable, style, fullscreen, visible, vsync, display, screen, config, context, mode)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mdisplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/__init__.py\u001b[0m in \u001b[0;36mget_display\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# Otherwise, create a new display and return it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyglet/canvas/xlib.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, x_screen)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXOpenDisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNoSuchDisplayException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot connect to \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mscreen_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXScreenCount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNoSuchDisplayException\u001b[0m: Cannot connect to \"None\""
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "agent = PPO(env, num_features=4, num_actions=2, gamma=.98, lam=1, epsilon=.2,\n",
    "            value_network_lr=0.001, policy_network_lr=.01, value_network_hidden_size=100,\n",
    "            policy_network_hidden_size_1=40, policy_network_hidden_size_2=35, policy_network_hidden_size_3=30)\n",
    "agent.run_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Cj4uyV1sBWX"
   },
   "source": [
    "# SOLVED\n",
    "\n",
    "## Cannot connect to none error apperaring because Google collab doesnt have runtime to start game "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PPO.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
